{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Annot8r.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Please uncomment the following code if you are running this notebook in a Google Colab virtual environment:"
      ],
      "metadata": {
        "id": "a9dE1e5-TcL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dataset\n",
        "'''\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive');\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/Data');\n",
        "'''"
      ],
      "metadata": {
        "id": "DnCcvumQZc1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe7db265-d4a9-44ab-94af-fca88d6b9a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i54UVu_KZSb-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "\n",
        "model = tf.keras.models.load_model('currentModel')\n",
        "#print(model.evaluate(test_ds, verbose=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )"
      ],
      "metadata": {
        "id": "FzN2QurBbwTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model constants.\n",
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")"
      ],
      "metadata": {
        "id": "0d88ThZWbs9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Please enter the sentence you would like to classify as an argumentative element\")\n",
        "input_raw = input()\n",
        "input = np.array([input_raw, 1])\n",
        "\n",
        "#input = np.array([\"Although cell phones caused my mother to leave, most phones are bad\", 1])\n",
        "#input = np.array([\"They're not the number one killer of mice in the U.S. as of 2009\", 1])\n",
        "#input = np.array([\"They're in my head they're all in my head oh god oh jesus\", 1])\n",
        "#input = np.array([\"26% of cell phones on the moon are a negative health risk to Americans\"])\n",
        "#input = np.array([\"In conclusion, there is no way I'm moving anywhere near a cell tower after this.\", 1])\n",
        "\n",
        "def PreProcess(input):\n",
        "  numpy_train = (input,np.asarray(1).astype('int32'))\n",
        "\n",
        "  raw_train_ds = tf.data.Dataset.from_tensors(numpy_train)\n",
        "\n",
        "\n",
        "  vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        "  )\n",
        "\n",
        "  text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "  #print(text_ds.as_numpy_iterator())\n",
        "  vectorize_layer.adapt(text_ds)\n",
        "\n",
        "  train_ds = text_ds.map(vectorize_layer)\n",
        "\n",
        "\n",
        "\n",
        "  # Test it with `raw_test_ds`, which yields raw strings\n",
        "  result = model.predict(train_ds)[0]\n",
        "  #print(result)\n",
        "\n",
        "  tag_list = [\"Lead\",\"Position\",\"Claim\",\"Counterclaim\",\"Rebuttal\",\"Evidence\",\"Concluding Statement\"]\n",
        "\n",
        "  max_value = max(result)\n",
        "\n",
        "\n",
        "  print(tag_list[int(np.where(result==max_value)[0])])\n",
        "\n",
        "PreProcess(input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "SLSlh08bbegk",
        "outputId": "f9b1cb19-81f1-4905-8ce3-78c6cafa01a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2ecf86277f66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#input = np.array([\"Although cell phones caused my mother to leave, most phones are bad\", 1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#input = np.array([\"They're not the number one killer of mice in the U.S. as of 2009\", 1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    }
  ]
}